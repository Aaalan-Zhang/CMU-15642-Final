{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6hQMW686LTO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install -U sentence-transformers[onnx-gpu]\n",
        "!pip install onnx onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbWhX-itp-lq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/My_Drive')\n",
        "%cd \"/content/My_Drive/MyDrive/2025 Spring/15642\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOphLZ879Fez"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import traceback\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\n",
        "from sentence_transformers.cross_encoder.losses import ListMLELoss\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.training_args import CrossEncoderTrainingArguments\n",
        "import time\n",
        "from sentence_transformers import export_dynamic_quantized_onnx_model\n",
        "from datetime import datetime\n",
        "from transformers import BertForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGS7ygBD9Nyl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"***REMOVED***\"\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXexsOMnDFaQ"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK7qLiacCpXt"
      },
      "source": [
        "## Initial Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftCJK4pA9Oel",
        "outputId": "17ce9e0b-7f7d-4833-94b8-7f99ae54112a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model max length: 512\n",
            "Model num labels: 1\n",
            "Model is on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_name = \"tomaarsen/reranker-MiniLM-L12-gooaq-bce\" # 33.4M model trained on gooaq, converged at the end of the 1st epoch, Nanomsmarco R100 Mrr@10 about 0.53, precision F32\n",
        "# tomaarsen/reranker-msmarco-MiniLM-L12-H384-uncased-lambdaloss # 33.4M model trained on MS MARCO, converged at the beginning of the 1st epoch, precision F32, Nanomsmarco R100 Mrr@10 about 0.530746\n",
        "# cross-encoder/ms-marco-MiniLM-L6-v2 # 22.7M SOTA model trained on MS MARCO, converged at the beginning, Nanomsmarco R100 Mrr@10 about 0.54, precision F32, base model microsoft/MiniLM-L12-H384-uncased\n",
        "\n",
        "# Set the log level to INFO to get more information\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "NUM = 64\n",
        "train_batch_size = NUM\n",
        "eval_batch_size = NUM\n",
        "mini_batch_size = NUM\n",
        "num_epochs = 1\n",
        "max_docs = None\n",
        "respect_input_order = True  # Whether to respect the original order of documents\n",
        "\n",
        "#Retrieve Time\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# 1. Define our CrossEncoder model\n",
        "model = CrossEncoder(model_name, num_labels=1)\n",
        "\n",
        "# print(model.model)\n",
        "\n",
        "# # Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model max length:\", model.max_length)\n",
        "print(\"Model num labels:\", model.num_labels)\n",
        "print(\"Model is on device:\", next(model.parameters()).device)\n",
        "\n",
        "# 2. Load the MS MARCO dataset: https://huggingface.co/datasets/microsoft/ms_marco\n",
        "logging.info(\"Read train dataset\")\n",
        "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")\n",
        "\n",
        "def listwise_mapper(batch, max_docs: int | None = 10):\n",
        "    processed_queries = []\n",
        "    processed_docs = []\n",
        "    processed_labels = []\n",
        "\n",
        "    for query, passages_info in zip(batch[\"query\"], batch[\"passages\"]):\n",
        "        # Extract passages and labels\n",
        "        passages = passages_info[\"passage_text\"]\n",
        "        labels = passages_info[\"is_selected\"]\n",
        "\n",
        "        # Pair passages with labels and sort descending by label (positives first)\n",
        "        paired = sorted(zip(passages, labels), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Separate back to passages and labels\n",
        "        sorted_passages, sorted_labels = zip(*paired) if paired else ([], [])\n",
        "\n",
        "        # Filter queries without any positive labels\n",
        "        if max(sorted_labels) < 1.0:\n",
        "            continue\n",
        "\n",
        "        # Truncate to max_docs\n",
        "        if max_docs is not None:\n",
        "            sorted_passages = list(sorted_passages[:max_docs])\n",
        "            sorted_labels = list(sorted_labels[:max_docs])\n",
        "\n",
        "        processed_queries.append(query)\n",
        "        processed_docs.append(sorted_passages)\n",
        "        processed_labels.append(sorted_labels)\n",
        "\n",
        "    return {\n",
        "        \"query\": processed_queries,\n",
        "        \"docs\": processed_docs,\n",
        "        \"labels\": processed_labels,\n",
        "    }\n",
        "\n",
        "# Create a dataset with a \"query\" column with strings, a \"docs\" column with lists of strings,\n",
        "# and a \"labels\" column with lists of floats\n",
        "dataset = dataset.map(\n",
        "    lambda batch: listwise_mapper(batch=batch, max_docs=max_docs),\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Processing listwise samples\",\n",
        ")\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=1_000)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "logging.info(train_dataset)\n",
        "\n",
        "# 3. Define our training loss\n",
        "loss = ListMLELoss(model, mini_batch_size=mini_batch_size, respect_input_order=respect_input_order)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkDLVVvcBtNq",
        "outputId": "33b1cd2c-d498-4ab7-eb67-c013184d2f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "# 4. Define the evaluator. We use the CENanoBEIREvaluator, which is a light-weight evaluator for English reranking\n",
        "evaluator = CrossEncoderNanoBEIREvaluator(dataset_names=[\"msmarco\"], batch_size=eval_batch_size)\n",
        "# evaluator(model)\n",
        "\n",
        "# 5. Define the training arguments\n",
        "short_model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n",
        "run_name = f\"reranker-msmarco-v1.1-{short_model_name}-listmle-{TIME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "L2pI8iCn9R3k"
      },
      "outputs": [],
      "source": [
        "args = CrossEncoderTrainingArguments(\n",
        "    output_dir=f\"models/{run_name}\",\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_NanoBEIR_R100_mean_ndcg@10\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=250,\n",
        "    logging_first_step=True,\n",
        "    run_name=run_name,\n",
        "    seed=12,\n",
        "    save_on_each_node=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "bvGVoso0CGpO"
      },
      "outputs": [],
      "source": [
        "def profiling_evaluator(model, cpu: bool = False):\n",
        "    \"\"\"\n",
        "    Run evaluator(model) and report:\n",
        "      - wall-clock time\n",
        "      - (if GPU) peak *new* GPU memory in MB\n",
        "    \"\"\"\n",
        "    # 1) pick device\n",
        "    device = torch.device(\"cpu\") if cpu else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # 2) GPU: warm up & snapshot baseline\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()               # wait for any inflight ops\n",
        "        start_alloc = torch.cuda.memory_allocated()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # 3) time the evaluation\n",
        "    t0 = time.time()\n",
        "    results = evaluator(model)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    # 4) report\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(f\"Evaluator wall time: {t1 - t0:.2f} seconds\")\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        peak_alloc = torch.cuda.max_memory_allocated()\n",
        "        used_bytes = peak_alloc - start_alloc\n",
        "        used_mb    = used_bytes / (1024**2)\n",
        "        print(f\"Peak *new* GPU memory: {used_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"Running on CPU—skipping GPU memory stats.\")\n",
        "    print(\"-\"*50 + \"\\n\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOxZ9l3VFFOU"
      },
      "source": [
        "## Pruning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lyhFmGnH3na"
      },
      "source": [
        "### Unstructured Pruning Trainer Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "UPbZBwCbH0vl"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class PruningTrainer(CrossEncoderTrainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.eval_steps=50\n",
        "        self.weight_dropout=0.2\n",
        "        self.max_pruning_steps=500\n",
        "        self.step_counter = 0\n",
        "        self.masks = []\n",
        "        self.prune_step = 1\n",
        "        for blk in self.model.model.bert.encoder.layer:\n",
        "            w = blk.intermediate.dense.weight\n",
        "            mask = torch.ones_like(w, dtype=torch.bool, device=w.device)\n",
        "            self.masks.append(mask)\n",
        "        self.total = sum(p.numel() for p in self.model.model.parameters())\n",
        "\n",
        "\n",
        "    def training_step(self, model: CrossEncoder, inputs: dict, batch_size: int) -> torch.Tensor:\n",
        "        # run the normal forward/backward/optimizer.step()\n",
        "        loss = super().training_step(model, inputs, batch_size)\n",
        "\n",
        "        self.step_counter += 1\n",
        "        with torch.no_grad():\n",
        "            # Re‑apply the existing mask so zeroed weights stay zero\n",
        "            for blk, mask in zip(model.model.bert.encoder.layer, self.masks):\n",
        "                blk.intermediate.dense.weight.data.mul_(mask)\n",
        "\n",
        "            # For each eval step, expand the mask by dropping a random subset of **remaining** weights\n",
        "            if self.step_counter % self.eval_steps == 0 and self.step_counter <= self.max_pruning_steps:\n",
        "                print(f\"— Pruning at step {self.step_counter} —\")\n",
        "                new_masks = []\n",
        "                if self.prune_step % 2 != 0:\n",
        "                    for blk, old_mask in zip(model.model.bert.encoder.layer[:6], self.masks[:6]):\n",
        "                        w = blk.intermediate.dense.weight\n",
        "                        # randomly dropout only on the **currently unpruned** positions\n",
        "                        keep_prob = 1.0 - self.weight_dropout\n",
        "                        random_tensor = torch.rand_like(w)\n",
        "                        drop_mask = (random_tensor < keep_prob)  # True = keep, False = drop\n",
        "                        combined_mask = old_mask & drop_mask\n",
        "                        # apply combined_mask\n",
        "                        w.data.mul_(combined_mask)\n",
        "                        new_masks.append(combined_mask)\n",
        "\n",
        "                    for i in range(len(new_masks)):\n",
        "                        self.masks[i] = new_masks[i]\n",
        "                else:\n",
        "                    for blk, old_mask in zip(model.model.bert.encoder.layer[6:], self.masks[6:]):\n",
        "                        w = blk.intermediate.dense.weight\n",
        "                        # randomly dropout only on the **currently unpruned** positions\n",
        "                        keep_prob = 1.0 - self.weight_dropout\n",
        "                        random_tensor = torch.rand_like(w)\n",
        "                        drop_mask = (random_tensor < keep_prob)  # True = keep, False = drop\n",
        "                        combined_mask = old_mask & drop_mask\n",
        "                        # apply it\n",
        "                        w.data.mul_(combined_mask)\n",
        "                        new_masks.append(combined_mask)\n",
        "\n",
        "                    for i in range(len(new_masks)):\n",
        "                        self.masks[i+6] = new_masks[i]\n",
        "\n",
        "\n",
        "            if self.step_counter % self.eval_steps == 0:\n",
        "                # total   = sum(p.numel() for p in model.parameters())\n",
        "                nonzero = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
        "                print(f\"Total params: {self.total:,},   non-zero: {nonzero:,}\")\n",
        "                print(f\"Sparsity: {100 * (1 - nonzero / self.total):.2f}%\")\n",
        "\n",
        "        self.prune_step += 1\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create the trainer & start training\n",
        "trainer = PruningTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()\n",
        "# 8. Save the final model\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "try:\n",
        "    save_model_name = f\"./models/random_mask_{TIME}\"\n",
        "    torch.save(model, f\"{save_model_name}.pt\")\n",
        "    final_output_dir = save_model_name\n",
        "    model.save_pretrained(final_output_dir)\n",
        "except:\n",
        "    print(\"Saving Failure\")\n"
      ],
      "metadata": {
        "id": "LEmFC5PwwY-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiling_evaluator(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEGnRH7Y3QBX",
        "outputId": "861e0c20-10a6-4879-b42e-a43dd1b3d1cc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.96 seconds\n",
            "Peak *new* GPU memory: 132.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.49747865280923753,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.4883015873015873,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.567446414649366,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.49747865280923753,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.4883015873015873,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.567446414649366,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi0EsEF1LfVI"
      },
      "source": [
        "### Structured Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BGe3SEVCLitu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n",
        "\n",
        "class RandomPruningTrainer(CrossEncoderTrainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        keep_ratio: float = 0.95,\n",
        "        prune_every: int = 50,\n",
        "        max_prune_rounds: int = 4,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert 0.0 < keep_ratio <= 1.0\n",
        "        self.keep_ratio        = keep_ratio\n",
        "        self.prune_every       = prune_every\n",
        "        self.max_prune_rounds  = max_prune_rounds\n",
        "        self.step_counter      = 0\n",
        "        self.prune_round       = 0\n",
        "        # model config\n",
        "        cfg = self.model.model.config\n",
        "        self.hidden_size       = cfg.hidden_size\n",
        "        self.intermediate_size = cfg.intermediate_size  # original MLP size\n",
        "        self.total_param       = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def training_step(self, model: CrossEncoder, inputs: dict, batch_size: int) -> torch.Tensor:\n",
        "        # Standard forward/backward/optimizer\n",
        "        loss = super().training_step(model, inputs, batch_size)\n",
        "        self.step_counter += 1\n",
        "\n",
        "        # Check if it's time to prune\n",
        "        if (self.step_counter % self.prune_every == 0\n",
        "            and self.prune_round < self.max_prune_rounds):\n",
        "\n",
        "            print(f\"— Random pruning round {self.prune_round+1}/{self.max_prune_rounds} at step {self.step_counter} —\")\n",
        "            hf = model.model  # BertForSequenceClassification\n",
        "\n",
        "            # Determine which half to prune this round\n",
        "            num_layers = len(hf.bert.encoder.layer)\n",
        "            half = num_layers // 2\n",
        "            start = 0 if (self.prune_round % 2 == 0) else half\n",
        "            end   = start + half\n",
        "\n",
        "            # Apply random structured pruning to selected layers\n",
        "            for idx in range(start, end):\n",
        "                layer = hf.bert.encoder.layer[idx]\n",
        "                w_int = layer.intermediate.dense.weight.data  # [intermediate, hidden]\n",
        "                n_neurons = w_int.size(0)\n",
        "                k_keep = int(self.keep_ratio * n_neurons)\n",
        "                if k_keep < 1:\n",
        "                    continue\n",
        "\n",
        "                # Randomly select k_keep neuron indices\n",
        "                perm = torch.randperm(n_neurons, device=w_int.device)\n",
        "                keep_idx = perm[:k_keep].sort().values\n",
        "\n",
        "                # Rebuild intermediate.dense: hidden -> k_keep\n",
        "                new_int = nn.Linear(self.hidden_size, k_keep,\n",
        "                                    bias=layer.intermediate.dense.bias is not None)\n",
        "                new_int.weight.data = w_int[keep_idx]\n",
        "                if layer.intermediate.dense.bias is not None:\n",
        "                    bias = layer.intermediate.dense.bias.data\n",
        "                    new_int.bias.data = bias[keep_idx]\n",
        "                layer.intermediate.dense = new_int\n",
        "\n",
        "                # Rebuild output.dense: k_keep -> hidden\n",
        "                w_out = layer.output.dense.weight.data  # [hidden, intermediate]\n",
        "                new_out = nn.Linear(k_keep, self.hidden_size,\n",
        "                                    bias=layer.output.dense.bias is not None)\n",
        "                new_out.weight.data = w_out[:, keep_idx]\n",
        "                if layer.output.dense.bias is not None:\n",
        "                    new_out.bias.data = layer.output.dense.bias.data\n",
        "                layer.output.dense = new_out\n",
        "\n",
        "            # Log total parameters after pruning\n",
        "            total_params = sum(p.numel() for p in model.parameters())\n",
        "            print(f\"Total params now: {total_params:,}\")\n",
        "            print(f\"Sparsity: {100 * (1 - total_params / self.total_param):.2f}%\")\n",
        "\n",
        "            self.prune_round += 1\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY-nI9JtE-4T"
      },
      "outputs": [],
      "source": [
        "# 6. Create the trainer & start training\n",
        "trainer = RandomPruningTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()\n",
        "# 8. Save the final model\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "try:\n",
        "    save_model_name = f\"./models/sp_{TIME}\"\n",
        "    torch.save(model, f\"{save_model_name}.pt\")\n",
        "    final_output_dir = save_model_name\n",
        "    model.save_pretrained(final_output_dir)\n",
        "except:\n",
        "    print(\"Saving Failure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uaT-gPXNjjC"
      },
      "source": [
        "### Structured Pruning - L1 Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fDh2HW-ZNmrv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n",
        "\n",
        "class StructuredPruningL1Trainer(CrossEncoderTrainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        keep_ratio: float = 0.95,        # fraction of neurons to keep globally per round\n",
        "        prune_every: int = 300,           # prune every N steps\n",
        "        max_prune_rounds: int = 4,       # total number of pruning rounds\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert 0.0 < keep_ratio <= 1.0\n",
        "        self.keep_ratio       = keep_ratio\n",
        "        self.prune_every      = prune_every\n",
        "        self.max_prune_rounds = max_prune_rounds\n",
        "        self.step_counter     = 0\n",
        "        self.prune_round      = 0\n",
        "        # model config\n",
        "        cfg = self.model.model.config\n",
        "        self.hidden_size       = cfg.hidden_size\n",
        "        self.intermediate_size = cfg.intermediate_size\n",
        "        self.total_param       = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def training_step(self, model: CrossEncoder, inputs: dict, batch_size: int) -> torch.Tensor:\n",
        "        loss = super().training_step(model, inputs, batch_size)\n",
        "        self.step_counter += 1\n",
        "\n",
        "        # time to prune?\n",
        "        if (self.step_counter % self.prune_every == 0\n",
        "            and self.prune_round < self.max_prune_rounds):\n",
        "            print(f\"— Pruning round {self.prune_round+1} at step {self.step_counter} —\")\n",
        "            hf = model.model\n",
        "            num_layers = len(hf.bert.encoder.layer)\n",
        "            half = num_layers // 2\n",
        "\n",
        "            # decide half based on round parity: odd rounds -> first half, even -> second half\n",
        "            if self.prune_round % 2 == 0:\n",
        "                layer_idxs = range(0, half)\n",
        "            else:\n",
        "                layer_idxs = range(half, num_layers)\n",
        "\n",
        "            # Collect L1 scores only for selected layers\n",
        "            scores = []  # (layer_idx, neuron_idx, score)\n",
        "            for layer_idx in layer_idxs:\n",
        "                layer = hf.bert.encoder.layer[layer_idx]\n",
        "                w_int = layer.intermediate.dense.weight.data  # [inter, hidden]\n",
        "                neuron_scores = w_int.abs().sum(dim=1)\n",
        "                for n_idx, score in enumerate(neuron_scores.tolist()):\n",
        "                    scores.append((layer_idx, n_idx, score))\n",
        "\n",
        "            # Determine cutoff for this slice\n",
        "            scores_sorted = sorted(scores, key=lambda x: x[2])\n",
        "            total_neurons = len(scores_sorted)\n",
        "            num_keep = int(self.keep_ratio * total_neurons)\n",
        "            keep_set = set((l, n) for l, n, _ in scores_sorted[-num_keep:])\n",
        "\n",
        "            # Rebuild only selected layers\n",
        "            for layer_idx in layer_idxs:\n",
        "                layer = hf.bert.encoder.layer[layer_idx]\n",
        "                # neurons to keep in this layer\n",
        "                kept = sorted(n for (l, n) in keep_set if l == layer_idx)\n",
        "                if not kept:\n",
        "                    continue\n",
        "                k = len(kept)\n",
        "                # rebuild intermediate.dense\n",
        "                w_int = layer.intermediate.dense.weight.data\n",
        "                new_int = nn.Linear(self.hidden_size, k,\n",
        "                                    bias=layer.intermediate.dense.bias is not None)\n",
        "                new_int.weight.data = w_int[kept]\n",
        "                if layer.intermediate.dense.bias is not None:\n",
        "                    new_int.bias.data = layer.intermediate.dense.bias.data[kept]\n",
        "                layer.intermediate.dense = new_int\n",
        "                # rebuild output.dense\n",
        "                w_out = layer.output.dense.weight.data\n",
        "                new_out = nn.Linear(k, self.hidden_size,\n",
        "                                    bias=layer.output.dense.bias is not None)\n",
        "                new_out.weight.data = w_out[:, kept]\n",
        "                if layer.output.dense.bias is not None:\n",
        "                    new_out.bias.data = layer.output.dense.bias.data\n",
        "                layer.output.dense = new_out\n",
        "\n",
        "            # log removal in this round and cumulative sparsity\n",
        "            new_total = sum(p.numel() for p in model.parameters())\n",
        "            removed = self.total_param - new_total\n",
        "            sparsity = 100.0 * removed / self.total_param\n",
        "            print(f\"   params remaining: {new_total:,} (removed {removed:,}, sparsity {sparsity:.2f}% )\")\n",
        "\n",
        "            self.prune_round += 1\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uH_KK7MUAH0r",
        "outputId": "1573ad5b-6edb-49b2-a620-86675ead54de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/My_Drive/MyDrive/2025 Spring/15642/wandb/run-20250430_200727-ywf4phdn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers/runs/ywf4phdn' target=\"_blank\">reranker-msmarco-v1.1-reranker-MiniLM-L12-gooaq-bce-listmle-2025-04-30 20:07:21</a></strong> to <a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers' target=\"_blank\">https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers/runs/ywf4phdn' target=\"_blank\">https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers/runs/ywf4phdn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   9/1230 00:57 < 2:47:02, 0.12 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Nanomsmarco R100 Map</th>\n",
              "      <th>Nanomsmarco R100 Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Ndcg@10</th>\n",
              "      <th>Nanomsmarco R100 Base Map</th>\n",
              "      <th>Nanomsmarco R100 Base Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Base Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Map</th>\n",
              "      <th>Nanobeir R100 Mean Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Map</th>\n",
              "      <th>Nanobeir R100 Mean Base Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Ndcg@10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.528578</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.508196</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.465065</td>\n",
              "      <td>0.432031</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432031</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.402620</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.321585</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.217079</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.097054</td>\n",
              "      <td>0.432785</td>\n",
              "      <td>0.421190</td>\n",
              "      <td>0.502810</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432785</td>\n",
              "      <td>0.421190</td>\n",
              "      <td>0.502810</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8/16 00:01 < 00:01, 4.22 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-766cfc89cca1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2596\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2599\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3025\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3026\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     def evaluation_loop(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4072\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4073\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4074\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4075\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     ) -> EvalLoopOutput:\n\u001b[0;32m--> 472\u001b[0;31m         output = super().evaluation_loop(\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4267\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4268\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4269\u001b[0m             inputs_decode = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4481\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4482\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4483\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4484\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    404\u001b[0m         ):\n\u001b[1;32m    405\u001b[0m             \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_model_in_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/losses/PListMLELoss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mmini_batch_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             tokens = self.model.tokenizer(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mmini_batch_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m                 )\n\u001b[1;32m   2955\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2956\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2957\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         )\n\u001b[1;32m   3157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3159\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = StructuredPruningL1Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 8. Save the final model\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "try:\n",
        "    save_model_name = f\"./models/sp_L1_{TIME}\"\n",
        "    torch.save(model, f\"{save_model_name}.pt\")\n",
        "    final_output_dir = save_model_name\n",
        "    model.save_pretrained(final_output_dir)\n",
        "except:\n",
        "    print(\"Saving Failure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k_G0E3OnPIn"
      },
      "source": [
        "## Quantize INT-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gtviJ387hDp1"
      },
      "outputs": [],
      "source": [
        "model = CrossEncoder(model_name, num_labels=1)\n",
        "model.save_pretrained(\"./models/model_fp32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6r1wmKFhifF",
        "outputId": "77502463-5cf8-4f7a-dd75-67a4aaa7b4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.cross_encoder.CrossEncoder:No 'model.onnx' found in 'tomaarsen/reranker-MiniLM-L12-gooaq-bce'. Exporting the model to ONNX.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:505 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.cross_encoder.CrossEncoder:Saving the exported ONNX model is heavily recommended to avoid having to export it again. Do so with `model.push_to_hub('tomaarsen/reranker-MiniLM-L12-gooaq-bce', create_pr=True)`.\n"
          ]
        }
      ],
      "source": [
        "model = CrossEncoder(\"tomaarsen/reranker-MiniLM-L12-gooaq-bce\", backend=\"onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJc0nR-nhBbR",
        "outputId": "28e0743c-87b3-4614-de35-e27b9c16d0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The ONNX file model_qint8_avx512_vnni.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:505 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n"
          ]
        }
      ],
      "source": [
        "export_dynamic_quantized_onnx_model(model, \"avx512_vnni\", \"./models/model_fp32\")\n",
        "# adjust accordingly\n",
        "model_int8 = CrossEncoder(\"./models/model_fp32\",\n",
        "                          backend=\"onnx\",\n",
        "                          model_kwargs={\"file_name\": \"onnx/model_qint8_avx512_vnni.onnx\"},)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74tFTybZhvTw",
        "outputId": "dda193f2-a209-47f7-b646-0ad1068364c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 100.19 seconds\n",
            "Peak *new* GPU memory: 0.18 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "===============INT_8===============\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.4125383393409709,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.39785714285714285,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.4764925075110537,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.4125383393409709,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.39785714285714285,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.4764925075110537,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model_int8.to(\"cuda\")\n",
        "eval_result = profiling_evaluator(model=model_int8)\n",
        "print(\"=\" * 15 + \"INT_8\" + \"=\" * 15)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "05GojGmNCb5I"
      },
      "outputs": [],
      "source": [
        "model_int8.save_pretrained(\"./models/model_int8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VRdQVe2w2Gu"
      },
      "source": [
        "## Profile Pruning Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gUSEpnsXxl0h"
      },
      "outputs": [],
      "source": [
        "org_model = CrossEncoder(\"tomaarsen/reranker-MiniLM-L12-gooaq-bce\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPZ0WGADxqgB",
        "outputId": "474e8b7c-22a2-401a-8f5c-d9a5a19fd1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.98 seconds\n",
            "Peak *new* GPU memory: 132.96 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.4319972197140892,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.4205238095238096,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.5021975259243594,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.4319972197140892,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.4205238095238096,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.5021975259243594,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=org_model)\n",
        "print(\"-----------------------------------\")\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ksSs8485w54K"
      },
      "outputs": [],
      "source": [
        "model_pruned = CrossEncoder(\"./prune_result\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nc7CWzxxMUm",
        "outputId": "e57ccebb-c832-4534-8547-be70d7cccf22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.88 seconds\n",
            "Peak *new* GPU memory: 132.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.3370697432658286,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.31622222222222224,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.36475284664158353,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.3370697432658286,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.31622222222222224,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.36475284664158353,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_pruned)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcYVtT_Gavpp",
        "outputId": "58d90d05-9529-4ce9-ef57-1e6c051ea42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.96 seconds\n",
            "Peak *new* GPU memory: 132.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.3370697432658286,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.31622222222222224,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.36475284664158353,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.3370697432658286,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.31622222222222224,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.36475284664158353,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model_pruned = CrossEncoder(\"./models/unstructured_Lp\")\n",
        "eval_result = profiling_evaluator(model=model_pruned)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSlFn-acbHRU",
        "outputId": "03b088e6-ed42-460a-fdcb-c5d6b79920d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.90 seconds\n",
            "Peak *new* GPU memory: 128.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.41964578138168795,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.40404761904761904,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.47975764876280935,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.41964578138168795,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.40404761904761904,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.47975764876280935,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "pruned_cross: CrossEncoder = torch.load(\"./models/pruned_L1_v3.pt\", weights_only=False)\n",
        "results = profiling_evaluator(pruned_cross)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_cross: CrossEncoder = torch.load(\"./models/sp_2025-04-30 21:11:37.pt\", weights_only=False)\n",
        "results = profiling_evaluator(pruned_cross)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV75nm33V1gX",
        "outputId": "db67f0e8-8ba4-49cf-a414-23a721ba1289"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.96 seconds\n",
            "Peak *new* GPU memory: 122.17 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.49662572356799695,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.4818571428571428,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.534606804298613,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.49662572356799695,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.4818571428571428,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.534606804298613,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_cross: CrossEncoder = torch.load(\"./models/sp_L1_2025-04-30 19:14:17.pt\", weights_only=False)\n",
        "results = profiling_evaluator(pruned_cross)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBX54vzKWArx",
        "outputId": "f55ca3f5-b3d6-4058-bcac-da0c0421f55d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.92 seconds\n",
            "Peak *new* GPU memory: 128.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.1919384794449809,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.17188888888888887,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.23948594355135802,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.1919384794449809,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.17188888888888887,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.23948594355135802,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "76Qq9Cx_xVu2"
      },
      "outputs": [],
      "source": [
        "model_fp16 = copy.deepcopy(org_model)\n",
        "model_bf16 = copy.deepcopy(org_model)\n",
        "\n",
        "model_fp16 = model_fp16.half()\n",
        "model_bf16 = model_bf16.to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y6vw3nWxZzf",
        "outputId": "f854d8ef-eb64-44f8-a0c3-a739224a8be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.23 seconds\n",
            "Peak *new* GPU memory: 65.57 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.4269972197140892,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.4271904761904761,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.5045274715625427,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.4269972197140892,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.4271904761904761,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.5045274715625427,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_fp16)\n",
        "print(\"-----------------------------------\")\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVXmfz7Uy2Hq",
        "outputId": "ea6788d0-bb44-4105-f6f9-49fd568111f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.14 seconds\n",
            "Peak *new* GPU memory: 65.57 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.33455546869093944,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.38126984126984126,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.47764309146703093,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.33455546869093944,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.38126984126984126,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.47764309146703093,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_bf16)\n",
        "print(\"-----------------------------------\")\n",
        "eval_result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JK7qLiacCpXt",
        "7lyhFmGnH3na",
        "FtQItt7QNulA"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}