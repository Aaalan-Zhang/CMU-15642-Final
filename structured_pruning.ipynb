{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6hQMW686LTO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install -U sentence-transformers[onnx-gpu]\n",
        "!pip install onnx onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWhX-itp-lq",
        "outputId": "fef1f742-678c-44e9-ca5d-c9b85860a91b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/My_Drive; to attempt to forcibly remount, call drive.mount(\"/content/My_Drive\", force_remount=True).\n",
            "/content/My_Drive/MyDrive/2025 Spring/15642\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/My_Drive')\n",
        "%cd \"/content/My_Drive/MyDrive/2025 Spring/15642\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zOphLZ879Fez"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import traceback\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\n",
        "from sentence_transformers.cross_encoder.losses import ListMLELoss\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.training_args import CrossEncoderTrainingArguments\n",
        "import time\n",
        "from sentence_transformers import export_dynamic_quantized_onnx_model\n",
        "from datetime import datetime\n",
        "from transformers import BertForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGS7ygBD9Nyl",
        "outputId": "acc34868-a094-4bbf-d3c8-14b26be21eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malanvoid\u001b[0m (\u001b[33malanvoid-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"***REMOVED***\"\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lXexsOMnDFaQ"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK7qLiacCpXt"
      },
      "source": [
        "## Initial Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftCJK4pA9Oel",
        "outputId": "beabb6a0-77b9-4c6b-b58d-401c49a6e903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model max length: 512\n",
            "Model num labels: 1\n",
            "Model is on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_name = \"tomaarsen/reranker-MiniLM-L12-gooaq-bce\" # 33.4M model trained on gooaq, converged at the end of the 1st epoch, Nanomsmarco R100 Mrr@10 about 0.53, precision F32\n",
        "# tomaarsen/reranker-msmarco-MiniLM-L12-H384-uncased-lambdaloss # 33.4M model trained on MS MARCO, converged at the beginning of the 1st epoch, precision F32, Nanomsmarco R100 Mrr@10 about 0.530746\n",
        "# cross-encoder/ms-marco-MiniLM-L6-v2 # 22.7M SOTA model trained on MS MARCO, converged at the beginning, Nanomsmarco R100 Mrr@10 about 0.54, precision F32, base model microsoft/MiniLM-L12-H384-uncased\n",
        "\n",
        "# Set the log level to INFO to get more information\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "NUM = 64\n",
        "train_batch_size = NUM\n",
        "eval_batch_size = NUM\n",
        "mini_batch_size = NUM\n",
        "num_epochs = 1\n",
        "max_docs = None\n",
        "respect_input_order = True  # Whether to respect the original order of documents\n",
        "\n",
        "#Retrieve Time\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# 1. Define our CrossEncoder model\n",
        "model = CrossEncoder(model_name, num_labels=1)\n",
        "\n",
        "# print(model.model)\n",
        "\n",
        "# # Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model max length:\", model.max_length)\n",
        "print(\"Model num labels:\", model.num_labels)\n",
        "print(\"Model is on device:\", next(model.parameters()).device)\n",
        "\n",
        "# 2. Load the MS MARCO dataset: https://huggingface.co/datasets/microsoft/ms_marco\n",
        "logging.info(\"Read train dataset\")\n",
        "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")\n",
        "\n",
        "def listwise_mapper(batch, max_docs: int | None = 10):\n",
        "    processed_queries = []\n",
        "    processed_docs = []\n",
        "    processed_labels = []\n",
        "\n",
        "    for query, passages_info in zip(batch[\"query\"], batch[\"passages\"]):\n",
        "        # Extract passages and labels\n",
        "        passages = passages_info[\"passage_text\"]\n",
        "        labels = passages_info[\"is_selected\"]\n",
        "\n",
        "        # Pair passages with labels and sort descending by label (positives first)\n",
        "        paired = sorted(zip(passages, labels), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Separate back to passages and labels\n",
        "        sorted_passages, sorted_labels = zip(*paired) if paired else ([], [])\n",
        "\n",
        "        # Filter queries without any positive labels\n",
        "        if max(sorted_labels) < 1.0:\n",
        "            continue\n",
        "\n",
        "        # Truncate to max_docs\n",
        "        if max_docs is not None:\n",
        "            sorted_passages = list(sorted_passages[:max_docs])\n",
        "            sorted_labels = list(sorted_labels[:max_docs])\n",
        "\n",
        "        processed_queries.append(query)\n",
        "        processed_docs.append(sorted_passages)\n",
        "        processed_labels.append(sorted_labels)\n",
        "\n",
        "    return {\n",
        "        \"query\": processed_queries,\n",
        "        \"docs\": processed_docs,\n",
        "        \"labels\": processed_labels,\n",
        "    }\n",
        "\n",
        "# Create a dataset with a \"query\" column with strings, a \"docs\" column with lists of strings,\n",
        "# and a \"labels\" column with lists of floats\n",
        "dataset = dataset.map(\n",
        "    lambda batch: listwise_mapper(batch=batch, max_docs=max_docs),\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Processing listwise samples\",\n",
        ")\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=1_000)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "logging.info(train_dataset)\n",
        "\n",
        "# 3. Define our training loss\n",
        "loss = ListMLELoss(model, mini_batch_size=mini_batch_size, respect_input_order=respect_input_order)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkDLVVvcBtNq",
        "outputId": "2e683d69-2546-4f29-eaaa-db15705f04d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "# 4. Define the evaluator. We use the CENanoBEIREvaluator, which is a light-weight evaluator for English reranking\n",
        "evaluator = CrossEncoderNanoBEIREvaluator(dataset_names=[\"msmarco\"], batch_size=eval_batch_size)\n",
        "# evaluator(model)\n",
        "\n",
        "# 5. Define the training arguments\n",
        "short_model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n",
        "run_name = f\"reranker-msmarco-v1.1-{short_model_name}-listmle-{TIME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L2pI8iCn9R3k"
      },
      "outputs": [],
      "source": [
        "args = CrossEncoderTrainingArguments(\n",
        "    output_dir=f\"models/{run_name}\",\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_NanoBEIR_R100_mean_ndcg@10\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=250,\n",
        "    logging_first_step=True,\n",
        "    run_name=run_name,\n",
        "    seed=12,\n",
        "    save_on_each_node=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bvGVoso0CGpO"
      },
      "outputs": [],
      "source": [
        "def profiling_evaluator(model, cpu: bool = False):\n",
        "    \"\"\"\n",
        "    Run evaluator(model) and report:\n",
        "      - wall-clock time\n",
        "      - (if GPU) peak *new* GPU memory in MB\n",
        "    \"\"\"\n",
        "    # 1) pick device\n",
        "    device = torch.device(\"cpu\") if cpu else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # 2) GPU: warm up & snapshot baseline\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()               # wait for any inflight ops\n",
        "        start_alloc = torch.cuda.memory_allocated()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # 3) time the evaluation\n",
        "    t0 = time.time()\n",
        "    results = evaluator(model)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    # 4) report\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(f\"Evaluator wall time: {t1 - t0:.2f} seconds\")\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        peak_alloc = torch.cuda.max_memory_allocated()\n",
        "        used_bytes = peak_alloc - start_alloc\n",
        "        used_mb    = used_bytes / (1024**2)\n",
        "        print(f\"Peak *new* GPU memory: {used_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"Running on CPU—skipping GPU memory stats.\")\n",
        "    print(\"-\"*50 + \"\\n\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOxZ9l3VFFOU"
      },
      "source": [
        "## Pruning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lyhFmGnH3na"
      },
      "source": [
        "### Pruning Trainer Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPbZBwCbH0vl"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class PruningTrainer(CrossEncoderTrainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.eval_steps=50\n",
        "        self.weight_dropout=0.2\n",
        "        self.max_pruning_steps=500\n",
        "        self.step_counter = 0\n",
        "        self.masks = []\n",
        "        self.prune_step = 1\n",
        "        for blk in self.model.model.bert.encoder.layer:\n",
        "            w = blk.intermediate.dense.weight\n",
        "            mask = torch.ones_like(w, dtype=torch.bool, device=w.device)\n",
        "            self.masks.append(mask)\n",
        "\n",
        "\n",
        "    def training_step(self, model: CrossEncoder, inputs: dict, batch_size: int) -> torch.Tensor:\n",
        "        # run the normal forward/backward/optimizer.step()\n",
        "        loss = super().training_step(model, inputs, batch_size)\n",
        "\n",
        "        self.step_counter += 1\n",
        "        with torch.no_grad():\n",
        "            # 2) ALWAYS re‑apply the existing mask so zeroed weights stay zero\n",
        "            for blk, mask in zip(model.model.bert.encoder.layer, self.masks):\n",
        "                blk.intermediate.dense.weight.data.mul_(mask)\n",
        "\n",
        "            # 3) every eval steps, expand the mask by dropping a random subset of *remaining* weights\n",
        "            if self.step_counter % self.eval_steps == 0 and self.step_counter <= self.max_pruning_steps:\n",
        "                print(f\"— Pruning at step {self.step_counter} —\")\n",
        "                new_masks = []\n",
        "                if self.prune_step % 2 != 0:\n",
        "                    for blk, old_mask in zip(model.model.bert.encoder.layer[:6], self.masks[:6]):\n",
        "                        w = blk.intermediate.dense.weight\n",
        "                        # randomly dropout only on the **currently unpruned** positions\n",
        "                        keep_prob = 1.0 - self.weight_dropout\n",
        "                        random_tensor = torch.rand_like(w)\n",
        "                        drop_mask = (random_tensor < keep_prob)  # True = keep, False = drop\n",
        "                        combined_mask = old_mask & drop_mask\n",
        "                        # apply it\n",
        "                        w.data.mul_(combined_mask)\n",
        "                        new_masks.append(combined_mask)\n",
        "\n",
        "                    for i in range(len(new_masks)):\n",
        "                        self.masks[i] = new_masks[i]\n",
        "                else:\n",
        "                    for blk, old_mask in zip(model.model.bert.encoder.layer[6:], self.masks[6:]):\n",
        "                        w = blk.intermediate.dense.weight\n",
        "                        # randomly dropout only on the **currently unpruned** positions\n",
        "                        keep_prob = 1.0 - self.weight_dropout\n",
        "                        random_tensor = torch.rand_like(w)\n",
        "                        drop_mask = (random_tensor < keep_prob)  # True = keep, False = drop\n",
        "                        combined_mask = old_mask & drop_mask\n",
        "                        # apply it\n",
        "                        w.data.mul_(combined_mask)\n",
        "                        new_masks.append(combined_mask)\n",
        "\n",
        "                    for i in range(len(new_masks)):\n",
        "                        self.masks[i+6] = new_masks[i]\n",
        "\n",
        "\n",
        "            if self.step_counter % self.eval_steps == 0:\n",
        "                total   = sum(p.numel() for p in model.parameters())\n",
        "                nonzero = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
        "                print(f\"   total params: {total:,},   non-zero: {nonzero:,}\")\n",
        "                print(f\"   sparsity: {100 * (1 - nonzero / total):.2f}%\")\n",
        "\n",
        "        self.prune_step += 1\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi0EsEF1LfVI"
      },
      "source": [
        "### Structured Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BGe3SEVCLitu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n",
        "\n",
        "class StructuredPruningTrainer(CrossEncoderTrainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        keep_ratio: float = 0.95,        # fraction of neurons to keep\n",
        "        prune_every: int = 50,           # prune every N steps\n",
        "        max_prune_rounds: int = 4,       # total number of alternating rounds\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        assert 0.0 < keep_ratio <= 1.0\n",
        "        self.keep_ratio       = keep_ratio\n",
        "        self.prune_every      = prune_every\n",
        "        self.max_prune_rounds = max_prune_rounds\n",
        "        self.step_counter     = 0\n",
        "        self.prune_round      = 0\n",
        "\n",
        "        # cache config\n",
        "        cfg = self.model.model.config\n",
        "        self.hidden_size      = cfg.hidden_size\n",
        "        self.intermediate_size = cfg.intermediate_size\n",
        "        self.total_param = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def training_step(self, model: CrossEncoder, inputs: dict, batch_size: int) -> torch.Tensor:\n",
        "        # 1) standard forward/backward/optimizer.step()\n",
        "        loss = super().training_step(model, inputs, batch_size)\n",
        "        self.step_counter += 1\n",
        "\n",
        "        # 2) structured prune at intervals\n",
        "        if (self.step_counter % self.prune_every == 0\n",
        "            and self.prune_round < self.max_prune_rounds):\n",
        "\n",
        "            # decide which half to prune\n",
        "            layer_start = 0 if (self.prune_round % 2 == 0) else 6\n",
        "            layer_end   = layer_start + 6\n",
        "            print(f\"— Pruning round {self.prune_round+1}/{self.max_prune_rounds} \"\n",
        "                  f\"(layers {layer_start}–{layer_end-1}) at step {self.step_counter} —\")\n",
        "\n",
        "            hf: nn.Module = model.model  # BertForSequenceClassification\n",
        "\n",
        "\n",
        "            # prune selected layers\n",
        "            for idx in range(layer_start, layer_end):\n",
        "                layer = hf.bert.encoder.layer[idx]\n",
        "\n",
        "                # importance score per neuron\n",
        "                w_int  = layer.intermediate.dense.weight.data    # [inter, hidden]\n",
        "                scores = w_int.abs().sum(dim=1)                  # [inter]\n",
        "                k = int(self.keep_ratio * scores.numel())\n",
        "                if k < 1:\n",
        "                    continue\n",
        "\n",
        "                # select top-k neurons\n",
        "                topk_idx = scores.topk(k).indices.sort().values\n",
        "\n",
        "                # rebuild intermediate.dense\n",
        "                new_int = nn.Linear(self.hidden_size, k,\n",
        "                                    bias=layer.intermediate.dense.bias is not None)\n",
        "                new_int.weight.data = w_int[topk_idx]\n",
        "                if layer.intermediate.dense.bias is not None:\n",
        "                    new_int.bias.data = layer.intermediate.dense.bias.data[topk_idx]\n",
        "                layer.intermediate.dense = new_int\n",
        "\n",
        "                # rebuild output.dense\n",
        "                w_out = layer.output.dense.weight.data           # [hidden, inter]\n",
        "                new_out = nn.Linear(k, self.hidden_size,\n",
        "                                    bias=layer.output.dense.bias is not None)\n",
        "                new_out.weight.data = w_out[:, topk_idx]\n",
        "                if layer.output.dense.bias is not None:\n",
        "                    new_out.bias.data = layer.output.dense.bias.data\n",
        "                layer.output.dense = new_out\n",
        "\n",
        "            # record new param count and sparsity\n",
        "            new_total = sum(p.numel() for p in model.parameters())\n",
        "            removed = self.total_param - new_total\n",
        "            sparsity = 100.0 * removed / self.total_param\n",
        "\n",
        "            print(f\"   total params: { self.total_param:,} (removed {removed:,}, sparsity {sparsity:.2f}% )\")\n",
        "\n",
        "            self.prune_round += 1\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY-nI9JtE-4T"
      },
      "outputs": [],
      "source": [
        "# 6. Create the trainer & start training\n",
        "trainer = StructuredPruningTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()\n",
        "# 8. Save the final model\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "try:\n",
        "    save_model_name = f\"./models/sp_{TIME}\"\n",
        "    torch.save(model, f\"{save_model_name}.pt\")\n",
        "    final_output_dir = save_model_name\n",
        "    model.save_pretrained(final_output_dir)\n",
        "except:\n",
        "    print(\"Saving Failure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uaT-gPXNjjC"
      },
      "source": [
        "### Structured Pruning - L1 Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fDh2HW-ZNmrv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n",
        "\n",
        "class StructuredPruningL1Trainer(CrossEncoderTrainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        keep_ratio: float = 0.95,        # fraction of neurons to keep globally per round\n",
        "        prune_every: int = 300,           # prune every N steps\n",
        "        max_prune_rounds: int = 4,       # total number of pruning rounds\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert 0.0 < keep_ratio <= 1.0\n",
        "        self.keep_ratio       = keep_ratio\n",
        "        self.prune_every      = prune_every\n",
        "        self.max_prune_rounds = max_prune_rounds\n",
        "        self.step_counter     = 0\n",
        "        self.prune_round      = 0\n",
        "        # model config\n",
        "        cfg = self.model.model.config\n",
        "        self.hidden_size       = cfg.hidden_size\n",
        "        self.intermediate_size = cfg.intermediate_size\n",
        "        self.total_param       = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def training_step(self, model: CrossEncoder, inputs: dict, batch_size: int) -> torch.Tensor:\n",
        "        loss = super().training_step(model, inputs, batch_size)\n",
        "        self.step_counter += 1\n",
        "\n",
        "        # time to prune?\n",
        "        if (self.step_counter % self.prune_every == 0\n",
        "            and self.prune_round < self.max_prune_rounds):\n",
        "\n",
        "            print(f\"— Pruning round {self.prune_round+1} at step {self.step_counter} —\")\n",
        "            hf = model.model  # BertForSequenceClassification\n",
        "            num_layers = len(hf.bert.encoder.layer)\n",
        "            half = num_layers // 2\n",
        "            # decide half based on round parity: odd rounds -> first half, even -> second half\n",
        "            if self.prune_round % 2 == 0:\n",
        "                layer_idxs = range(0, half)\n",
        "            else:\n",
        "                layer_idxs = range(half, num_layers)\n",
        "\n",
        "            # Collect L1 scores only for selected layers\n",
        "            scores = []  # (layer_idx, neuron_idx, score)\n",
        "            for layer_idx in layer_idxs:\n",
        "                layer = hf.bert.encoder.layer[layer_idx]\n",
        "                w_int = layer.intermediate.dense.weight.data  # [inter, hidden]\n",
        "                neuron_scores = w_int.abs().sum(dim=1)\n",
        "                for n_idx, score in enumerate(neuron_scores.tolist()):\n",
        "                    scores.append((layer_idx, n_idx, score))\n",
        "\n",
        "            # Determine cutoff for this slice\n",
        "            scores_sorted = sorted(scores, key=lambda x: x[2])\n",
        "            total_neurons = len(scores_sorted)\n",
        "            num_keep = int(self.keep_ratio * total_neurons)\n",
        "            keep_set = set((l, n) for l, n, _ in scores_sorted[-num_keep:])\n",
        "\n",
        "            # Rebuild only selected layers\n",
        "            for layer_idx in layer_idxs:\n",
        "                layer = hf.bert.encoder.layer[layer_idx]\n",
        "                # neurons to keep in this layer\n",
        "                kept = sorted(n for (l, n) in keep_set if l == layer_idx)\n",
        "                if not kept:\n",
        "                    continue\n",
        "                k = len(kept)\n",
        "                # rebuild intermediate.dense\n",
        "                w_int = layer.intermediate.dense.weight.data\n",
        "                new_int = nn.Linear(self.hidden_size, k,\n",
        "                                    bias=layer.intermediate.dense.bias is not None)\n",
        "                new_int.weight.data = w_int[kept]\n",
        "                if layer.intermediate.dense.bias is not None:\n",
        "                    new_int.bias.data = layer.intermediate.dense.bias.data[kept]\n",
        "                layer.intermediate.dense = new_int\n",
        "                # rebuild output.dense\n",
        "                w_out = layer.output.dense.weight.data\n",
        "                new_out = nn.Linear(k, self.hidden_size,\n",
        "                                    bias=layer.output.dense.bias is not None)\n",
        "                new_out.weight.data = w_out[:, kept]\n",
        "                if layer.output.dense.bias is not None:\n",
        "                    new_out.bias.data = layer.output.dense.bias.data\n",
        "                layer.output.dense = new_out\n",
        "\n",
        "            # log removal in this round and cumulative sparsity\n",
        "            new_total = sum(p.numel() for p in model.parameters())\n",
        "            removed = self.total_param - new_total\n",
        "            sparsity = 100.0 * removed / self.total_param\n",
        "            print(f\"   params remaining: {new_total:,} (removed {removed:,}, sparsity {sparsity:.2f}% )\")\n",
        "\n",
        "            self.prune_round += 1\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "uH_KK7MUAH0r",
        "outputId": "1573ad5b-6edb-49b2-a620-86675ead54de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/My_Drive/MyDrive/2025 Spring/15642/wandb/run-20250430_200727-ywf4phdn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers/runs/ywf4phdn' target=\"_blank\">reranker-msmarco-v1.1-reranker-MiniLM-L12-gooaq-bce-listmle-2025-04-30 20:07:21</a></strong> to <a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers' target=\"_blank\">https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers/runs/ywf4phdn' target=\"_blank\">https://wandb.ai/alanvoid-carnegie-mellon-university/sentence-transformers/runs/ywf4phdn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   9/1230 00:57 < 2:47:02, 0.12 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Nanomsmarco R100 Map</th>\n",
              "      <th>Nanomsmarco R100 Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Ndcg@10</th>\n",
              "      <th>Nanomsmarco R100 Base Map</th>\n",
              "      <th>Nanomsmarco R100 Base Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Base Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Map</th>\n",
              "      <th>Nanobeir R100 Mean Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Map</th>\n",
              "      <th>Nanobeir R100 Mean Base Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Ndcg@10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.528578</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.508196</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.431997</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.465065</td>\n",
              "      <td>0.432031</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432031</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.402620</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.321585</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432027</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.217079</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432085</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>18.651000</td>\n",
              "      <td>18.097054</td>\n",
              "      <td>0.432785</td>\n",
              "      <td>0.421190</td>\n",
              "      <td>0.502810</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.432785</td>\n",
              "      <td>0.421190</td>\n",
              "      <td>0.502810</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8/16 00:01 < 00:01, 4.22 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-766cfc89cca1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2596\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2599\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3025\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3026\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     def evaluation_loop(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4072\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4073\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4074\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4075\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     ) -> EvalLoopOutput:\n\u001b[0;32m--> 472\u001b[0;31m         output = super().evaluation_loop(\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4267\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4268\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4269\u001b[0m             inputs_decode = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4481\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4482\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4483\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4484\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    404\u001b[0m         ):\n\u001b[1;32m    405\u001b[0m             \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_model_in_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/losses/PListMLELoss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mmini_batch_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             tokens = self.model.tokenizer(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mmini_batch_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m                 )\n\u001b[1;32m   2955\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2956\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2957\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         )\n\u001b[1;32m   3157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3159\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = StructuredPruningL1Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 8. Save the final model\n",
        "TIME = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "try:\n",
        "    save_model_name = f\"./models/sp_L1_{TIME}\"\n",
        "    torch.save(model, f\"{save_model_name}.pt\")\n",
        "    final_output_dir = save_model_name\n",
        "    model.save_pretrained(final_output_dir)\n",
        "except:\n",
        "    print(\"Saving Failure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtQItt7QNulA"
      },
      "source": [
        "### Tuning Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uIUDf0ZZLl2"
      },
      "outputs": [],
      "source": [
        "args = CrossEncoderTrainingArguments(\n",
        "    output_dir=f\"models/{run_name}\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    learning_rate=2e-8,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_NanoBEIR_R100_mean_ndcg@10\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=250,\n",
        "    logging_first_step=True,\n",
        "    run_name=run_name,\n",
        "    seed=12,\n",
        "    half_precision_backend=\"cuda_amp\",\n",
        "    fp16_full_eval=True,\n",
        "    gradient_accumulation_steps = 4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Hyx8aTNyVA",
        "outputId": "49034a8c-589b-42a3-c435-a606d7ad8d45"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='307' max='307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [307/307 11:06, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Nanomsmarco R100 Map</th>\n",
              "      <th>Nanomsmarco R100 Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Ndcg@10</th>\n",
              "      <th>Nanomsmarco R100 Base Map</th>\n",
              "      <th>Nanomsmarco R100 Base Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Base Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Map</th>\n",
              "      <th>Nanobeir R100 Mean Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Map</th>\n",
              "      <th>Nanobeir R100 Mean Base Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Ndcg@10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.211517</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.205910</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.213312</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.213287</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.213348</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.211681</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.212568</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.212132</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>44.803900</td>\n",
              "      <td>11.213408</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>44.834300</td>\n",
              "      <td>11.212613</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>44.834300</td>\n",
              "      <td>11.211847</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>44.834300</td>\n",
              "      <td>11.208872</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.367612</td>\n",
              "      <td>0.435659</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the best model at models/reranker-msmarco-v1.1-reranker-MiniLM-L12-gooaq-bce-listmle/checkpoint-25/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
          ]
        }
      ],
      "source": [
        "# 6. Create the trainer & start training\n",
        "trainer = CrossEncoderTrainer(\n",
        "    model=model_fp16,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ycjgphlfInm",
        "outputId": "19611295-1cec-45bd-a6d2-84bedb663650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-----------------------------------\n",
            "Evaluator inference time: 3.08 seconds\n",
            "The Peak GPU memory usage during evaluation: 67.25 MB\n",
            "===============FP_16===============\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.367611621170746,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.4356587301587302,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.5352657958246084,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.367611621170746,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.4356587301587302,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.5352657958246084,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_fp16)\n",
        "print(\"=\" * 15 + \"FP_16\" + \"=\" * 15)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hiNdrywZbT3"
      },
      "outputs": [],
      "source": [
        "args = CrossEncoderTrainingArguments(\n",
        "    output_dir=f\"models/{run_name}\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    learning_rate=2e-8,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
        "    bf16_full_eval=True,\n",
        "    half_precision_backend=\"cuda_amp\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_NanoBEIR_R100_mean_ndcg@10\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=250,\n",
        "    logging_first_step=True,\n",
        "    run_name=run_name,\n",
        "    seed=12,\n",
        "    gradient_accumulation_steps = 4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO9rRXXFZGAT",
        "outputId": "8678f443-91b7-4b7e-845b-48379926f696"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='307' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 65/307 02:06 < 08:04, 0.50 it/s, Epoch 0.21/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Nanomsmarco R100 Map</th>\n",
              "      <th>Nanomsmarco R100 Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Ndcg@10</th>\n",
              "      <th>Nanomsmarco R100 Base Map</th>\n",
              "      <th>Nanomsmarco R100 Base Mrr@10</th>\n",
              "      <th>Nanomsmarco R100 Base Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Map</th>\n",
              "      <th>Nanobeir R100 Mean Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Ndcg@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Map</th>\n",
              "      <th>Nanobeir R100 Mean Base Mrr@10</th>\n",
              "      <th>Nanobeir R100 Mean Base Ndcg@10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>44.813500</td>\n",
              "      <td>11.211983</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.260436</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.260436</td>\n",
              "      <td>0.489577</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.540426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-fa384cf8f312>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    404\u001b[0m         ):\n\u001b[1;32m    405\u001b[0m             \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_model_in_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/losses/PListMLELoss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mmini_batch_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             tokens = self.model.tokenizer(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mmini_batch_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m                 )\n\u001b[1;32m   2955\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2956\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2957\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         )\n\u001b[1;32m   3157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3159\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     def _encode_plus(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     \u001b[0;31m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtensor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 6. Create the trainer & start training\n",
        "trainer = CrossEncoderTrainer(\n",
        "    model=model_bf16,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "\n",
        ")\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k_G0E3OnPIn"
      },
      "source": [
        "## Quantize INT-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gtviJ387hDp1"
      },
      "outputs": [],
      "source": [
        "model = CrossEncoder(model_name, num_labels=1)\n",
        "model.save_pretrained(\"./models/model_fp32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f6r1wmKFhifF",
        "outputId": "771188c7-a3a0-4332-92a7-e9d54927f7d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.cross_encoder.CrossEncoder:No 'model.onnx' found in 'tomaarsen/reranker-MiniLM-L12-gooaq-bce'. Exporting the model to ONNX.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:505 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.cross_encoder.CrossEncoder:Saving the exported ONNX model is heavily recommended to avoid having to export it again. Do so with `model.push_to_hub('tomaarsen/reranker-MiniLM-L12-gooaq-bce', create_pr=True)`.\n"
          ]
        }
      ],
      "source": [
        "model = CrossEncoder(\"tomaarsen/reranker-MiniLM-L12-gooaq-bce\", backend=\"onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJc0nR-nhBbR",
        "outputId": "584eb427-dbbe-4b38-9ffc-df86ab7154c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The ONNX file model_qint8_avx512_vnni.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:505 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n"
          ]
        }
      ],
      "source": [
        "export_dynamic_quantized_onnx_model(model, \"avx512_vnni\", \"./models/model_fp32\")\n",
        "# adjust accordingly\n",
        "model_int8 = CrossEncoder(\"./models/model_fp32\",\n",
        "                          backend=\"onnx\",\n",
        "                          model_kwargs={\"file_name\": \"onnx/model_qint8_avx512_vnni.onnx\"},)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74tFTybZhvTw",
        "outputId": "11f5e967-cdd1-4f9d-a092-77d1103d3e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 99.96 seconds\n",
            "Peak *new* GPU memory: 0.18 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "===============INT_8===============\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.4125383393409709,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.39785714285714285,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.4764925075110537,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.4125383393409709,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.39785714285714285,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.4764925075110537,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_int8)\n",
        "print(\"=\" * 15 + \"INT_8\" + \"=\" * 15)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "05GojGmNCb5I"
      },
      "outputs": [],
      "source": [
        "model_int8.save_pretrained(\"./models/model_int8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VRdQVe2w2Gu"
      },
      "source": [
        "## Profile Pruning Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUSEpnsXxl0h"
      },
      "outputs": [],
      "source": [
        "org_model = CrossEncoder(\"tomaarsen/reranker-MiniLM-L12-gooaq-bce\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPZ0WGADxqgB",
        "outputId": "a566e49a-247c-46c6-eb2c-e445b81d68a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.81 seconds\n",
            "Peak *new* GPU memory: 132.96 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.4319972197140892,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.4205238095238096,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.5021975259243594,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.4319972197140892,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.4205238095238096,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.5021975259243594,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=org_model)\n",
        "print(\"-----------------------------------\")\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksSs8485w54K"
      },
      "outputs": [],
      "source": [
        "model_pruned = CrossEncoder(\"./prune_result\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nc7CWzxxMUm",
        "outputId": "27ef826e-894b-4f5d-80ba-a38b83ab6154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.83 seconds\n",
            "Peak *new* GPU memory: 132.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.3370697432658286,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.31622222222222224,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.36475284664158353,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.3370697432658286,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.31622222222222224,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.36475284664158353,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_pruned)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcYVtT_Gavpp",
        "outputId": "2f79e3b5-1bec-46d8-a723-fad2fd8c50a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.85 seconds\n",
            "Peak *new* GPU memory: 144.44 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.3370697432658286,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.31622222222222224,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.36475284664158353,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.3370697432658286,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.31622222222222224,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.36475284664158353,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_pruned = CrossEncoder(\"./models/unstructured_L1\")\n",
        "eval_result = profiling_evaluator(model=model_pruned)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OOmmEaia_xW",
        "outputId": "9e58bc56-0aa7-44f9-f120-f7f68ec64959"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.91 seconds\n",
            "Peak *new* GPU memory: 139.76 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.39317046503461534,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.38085714285714284,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.46463984383749674,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.39317046503461534,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.38085714285714284,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.46463984383749674,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from transformers import BertForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# 1) Load the pruned HuggingFace model + tokenizer (config was already patched)\n",
        "hf_model  = BertForSequenceClassification.from_pretrained(\"./models/structured_prune_v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./models/structured_prune_v1\")\n",
        "\n",
        "# 2) Create a CrossEncoder normally (so Module.__init__ runs)\n",
        "#    You just need to point it at any compatible name; we'll override the internals:\n",
        "model_pruned = CrossEncoder(\"bert-base-uncased\", num_labels=1)\n",
        "\n",
        "# 3) Now safely replace its internals\n",
        "model_pruned.model     = hf_model\n",
        "model_pruned.tokenizer = tokenizer\n",
        "model_pruned.max_length = hf_model.config.max_position_embeddings\n",
        "\n",
        "# 4) Move to device and evaluate\n",
        "model_pruned.to(device)\n",
        "eval_result = profiling_evaluator(model_pruned)\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSlFn-acbHRU",
        "outputId": "838d50b7-6ef6-4f03-9192-7eaaa16f3ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.82 seconds\n",
            "Peak *new* GPU memory: 128.96 MB\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.41964578138168795,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.40404761904761904,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.47975764876280935,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.41964578138168795,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.40404761904761904,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.47975764876280935,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2b) Load the CrossEncoder wrapper\n",
        "pruned_cross: CrossEncoder = torch.load(\"./models/pruned_L1_v3.pt\", weights_only=False)\n",
        "\n",
        "# 2d) Now you can run your evaluator or further training\n",
        "results = profiling_evaluator(pruned_cross)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Qq9Cx_xVu2"
      },
      "outputs": [],
      "source": [
        "model_fp16 = copy.deepcopy(model_pruned)\n",
        "model_bf16 = copy.deepcopy(model_pruned)\n",
        "\n",
        "model_fp16 = model_fp16.half()\n",
        "model_bf16 = model_bf16.to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y6vw3nWxZzf",
        "outputId": "9a40bfe8-9821-4c43-d983-e7a35acc200e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.17 seconds\n",
            "Peak *new* GPU memory: 65.57 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.1916345722605474,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.2633809523809523,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.3153104444448795,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.1916345722605474,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.2633809523809523,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.3153104444448795,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_fp16)\n",
        "print(\"-----------------------------------\")\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVXmfz7Uy2Hq",
        "outputId": "318f5057-6b32-4fb6-b354-5725c2d6d958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Evaluator wall time: 3.14 seconds\n",
            "Peak *new* GPU memory: 66.55 MB\n",
            "--------------------------------------------------\n",
            "\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'NanoMSMARCO_R100_map': 0.11461818181818181,\n",
              " 'NanoMSMARCO_R100_mrr@10': 0.15285714285714286,\n",
              " 'NanoMSMARCO_R100_ndcg@10': 0.19392612688849528,\n",
              " 'NanoMSMARCO_R100_base_map': 0.4895766320756843,\n",
              " 'NanoMSMARCO_R100_base_mrr@10': 0.4775,\n",
              " 'NanoMSMARCO_R100_base_ndcg@10': 0.5404259879670522,\n",
              " 'NanoBEIR_R100_mean_map': 0.11461818181818181,\n",
              " 'NanoBEIR_R100_mean_mrr@10': 0.15285714285714286,\n",
              " 'NanoBEIR_R100_mean_ndcg@10': 0.19392612688849528,\n",
              " 'NanoBEIR_R100_mean_base_map': 0.4895766320756843,\n",
              " 'NanoBEIR_R100_mean_base_mrr@10': 0.4775,\n",
              " 'NanoBEIR_R100_mean_base_ndcg@10': 0.5404259879670522}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_result = profiling_evaluator(model=model_bf16)\n",
        "print(\"-----------------------------------\")\n",
        "eval_result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7lyhFmGnH3na",
        "bi0EsEF1LfVI",
        "7uaT-gPXNjjC",
        "6V8K1oF9H7Rj",
        "NfpQMzx6IQOL",
        "FtQItt7QNulA",
        "9k_G0E3OnPIn"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}